####packages
destroyX = function(es) {
  f = es
  for (col in c(1:ncol(f))){ #for each column in dataframe
    if (startsWith(colnames(f)[col], "X") == TRUE)  { #if starts with 'X' ..
      colnames(f)[col] <- substr(colnames(f)[col], 2, 100) #get rid of it
    }
  }
  assign(deparse(substitute(es)), f, inherits = TRUE) #assign corrected data to original name
}
library(reshape2)
library(lubridate)
library(xts)
library(tsbox)
library(zoo)
library(ggplot2)
library(caret)
###############Data frame reshape
setwd("C:\\Users\\ADMIN\\Desktop\\machine_learning")
runoff<-read.table("runoff.txt",header=T, na.strings = "NA", sep="\t")
runoff = destroyX(runoff)
R = melt(runoff,id.vars=c("Year","month"))
colnames(R) = c("Year","Month","Day","Q")
date = as.data.frame(with(R, ymd(paste(Year, Month, Day, sep= ' '))))
R1 = cbind(R,date)
R1 = na.omit(R1)
colnames(R1) = c("Year","Month","Day","Q","Date")
R1 = R1[order(as.Date(R1$Date, format="%d/%m/%Y")),]
R1[,4] <- sapply(R1[,4],as.numeric)
####add features to this time-series datasets
R1$yday = yday(R1$Date)
R1$quarter = quarter(R1$Date)
R1$weekdays = weekdays(R1$Date)
R1$QAVEM = with(R1,ave(R1$Q,R1$Month,FUN=mean)) # for calculate month mean,year mean
R1$QAVEY = with(R1,ave(R1$Q,R1$Year,FUN=mean))
R1$Qmonth = with(R1,ave(R1$Q,R1[,1:2],FUN=mean))
########################data frame to ts , for autoregression
TS = xts(x=R1[,"Q"], order.by=R1[,"Date"])
#m = na.StructTS(ts_ht, na.rm = FALSE, maxgap = Inf) to Interpolate NA 
TS = ts_ts(TS)
plot.ts(TS)

lag1day <- lag(TS, -1)
lag2day<- lag(TS, -2)
lag3day <- lag(TS, -3)
lag4day <- lag(TS, -4)
lag5day <- lag(TS, -5)
lag6day <- lag(TS, -6)
lag7day <- lag(TS, -7)
lag8day <- lag(TS, -8)
lag9day <- lag(TS, -9)
lag10day <- lag(TS, -10)

lagdata <- ts.intersect(TS, lag1day,lag2day, lag3day,
                        lag4day,lag5day, lag6day,
                        lag7day,lag8day, lag9day,lag10day,
                        dframe=T)

corR = cor(lagdata, method = "pearson", use = "complete.obs")
corR = as.data.frame(t(rbind(corR[1,2:11],seq(1:10))))
### you can plot to check the autoregression
ggplot(data=corR, aes(x=V2, y=V1)) +
  geom_line(linetype="dashed")+
  geom_point()+theme(
    axis.title=element_text(size=18,face="plain",color="black"),
    axis.text = element_text(size=18,face="plain",color="black"),
    legend.title=element_text(size=18,face="plain",color="black"),
    text=element_text(size=18,  family="Times"),
    legend.position = "right"# c(0.83,0.15)
  )+ scale_x_discrete(name ="Lag Days", 
                      limits=seq(1:10))+
  scale_y_continuous(limits=c(0.5,1),name="Autocorrelation")
####t-1.t-2,t-3 as variables to add the data frame############train Test
Lag123 = ts.intersect(TS, lag1day,lag2day, lag3day, dframe=T)
R1 = cbind(R1[4:8400,],Lag123)
R1 = na.omit(R1)
#####machine learning data for traning and testing
train = R1[R1$Date <= "2008-01-01",]
dt =cbind(train[,1:3],train[,6:7],train[,9:11],train[,13:15])
test = R1[R1$Date > "2008-01-01",]
test = test[test$Date <= "2011-12-31",]

#######################machine learning:Random Forest
set.seed(123)
library(randomForest)
RF_first = randomForest(Q ~ Year+Month+Day+yday+quarter+
                    lag1day+lag2day+lag3day+QAVEM+QAVEY+Qmonth,ntree = 500
                  ,data = train,importance = TRUE)
dt =cbind(train[,1:3],train[,6:7],train[,9:11],train[,13:15])###data have no Y
t <- tuneRF(dt, train$Q, # to ensure ntry parameter
            stepFactor = 0.5,
            plot = TRUE,
            ntreeTry = 500,
            trace = TRUE,
            improve = 0.05)
t = as.data.frame(t)
library(dplyr)
mtry_select = t[as.numeric(order(t$OOBError)[1]),]$mtry
RF = randomForest(Q ~ Year+Month+Day+yday+quarter+
                    lag1day+lag2day+lag3day+QAVEM+QAVEY+Qmonth,ntree = 500
                  ,data = train,mtry = mtry_select,importance = TRUE)
############final model by turnRF min OOb to mtry
################then calculate the RMSE (one month )
pred_y = predict(RF,dt)
caret::RMSE(test_y, pred_y)
RF$mse[length(RF$mse)]
rf_rmse = sqrt(RF$mse[length(RF$mse)])
#################boosting model XGBT
#########################################################machine learning:XGBT
library(xgboost)
train_x = data.matrix(cbind(train[,1:3],train[,6:7],train[,9:11],train[,13:15]))
train_y = train$Q
#define predictor and response variables in testing set
test_x = data.matrix(cbind(test[,1:3],test[,6:7],test[,9:11],test[,13:15]))
test_y = test$Q
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
watchlist = list(train=xgb_train, test=xgb_test)
#fit XGBoost model and display training and testing data at each round
XGBT = xgb.train(data = xgb_train, watchlist=watchlist, nrounds = 70)
final = xgboost(data = xgb_train,  nrounds = 10 , verbose = 0 )
pred_y = predict(final,test_x)
caret::RMSE(test_y, pred_y)
######################coplar models
#################################################################################
# Model information
# Model name: Bayesian vine copula (BVC) model
# Model developer: Zhiyong Liu  
# Contact:   liuzhiy25@mail.sysu.edu.cn  
# Version:   v1.0
# Language: R 
# Date: 2021-03-17
#article: Liu et al., A Hybrid Bayesian Vine Model for Water Level Prediction , 2021
#################################################################################


library(xts)
library(hydroTSM)
library(hydroGOF)
library(VineCopula)
library(fitdistrplus)
library(copula)
library(ensembleBMA)
library(CDVineCopulaConditional)

#develop a function  for fitting the best marginal distributions
best_fit_dist<-function (x,x1=10,x2=0.5) { 
  set.seed(1)
  library(fitdistrplus)
  
  
  require(MASS)
  dat=x
  normal<-tryCatch({  fitdist(dat,"norm",method = "mle")}, 
                   error = function(err) {return(NA)})
  
  gamma<-tryCatch({ fitdist(dat,"gamma",method = "mle")}, 
                  error = function(err) {return(NA)})
  
  lognormal<-tryCatch({  fitdist(dat,"lnorm",method = "mle")}, 
                      error = function(err) {return(NA)})
  weibull<-tryCatch({ fitdist(dat,"weibull",method = "mle")}, 
                    error = function(err) {return(NA)})
  
  
  
  fits=list( normal, gamma, lognormal,weibull )
  sim={}
  p_value={}
  for (i in 1:4 ) { 
    if   (!is.na (fits[[ i]][1])  ) {  
      #sim=cbind(sim, gofstat(fits[[ i]]  )$chisq )
      #sim=cbind(sim, gofstat(fits[[ i]]  )$chisq )
      sim =tryCatch({cbind(sim, gofstat(fits[[ i]]  )$chisq) },
                    error = function(err) {return(cbind(sim,-i))} )  
      
      
      #p_value =cbind(p_value, gofstat(fits[[ i]]  )$chisqpvalue )
      # p_value =tryCatch({cbind(p_value, gofstat(fits[[ i]]  )$chisqpvalue) },
      #                   error = function(err) {return(cbind(p_value,i))} )   
      
      
    } else {
      sim=cbind(sim, NA )
      #p_value=cbind(p_value,NA)
      
    }
  }
  
  chi=rbind(sim)
  
  
  colnames(chi)<- c('normal',"gamma", "lognormal",'Weibull' )
  
  #max_log=sapply(fits, function(i) i$loglik)
  #max_index<- which.max( max_log )
  max_index<- which.min( sim)
  
  #max_index=5
  paras=fits[[max_index]]
  if (max_index==1) {
    
    v_cdf=pnorm(x,mean=paras$estimate[1],sd=paras$estimate[2] )
    v1=pnorm(x1,mean=paras$estimate[1],sd=paras$estimate[2] )
    v2=qnorm(x2,mean=paras$estimate[1],sd=paras$estimate[2] )
    
  } else if (max_index==4) {
    v_cdf=pweibull(x,shape=paras$estimate[1],scale=paras$estimate[2] )
    v1=pweibull(x1,shape=paras$estimate[1],scale=paras$estimate[2] )
    v2=qweibull(x2,shape=paras$estimate[1],scale=paras$estimate[2] )
    
    
    
  }      else if (max_index==2) {
    
    v_cdf=pgamma(x,shape=paras$estimate[1],rate=paras$estimate[2] )
    v1=pgamma(x1,shape=paras$estimate[1],rate=paras$estimate[2] )
    v2=qgamma(x2,shape=paras$estimate[1],rate=paras$estimate[2] )
  }      else if (max_index==3) {
    
    v_cdf= plnorm(x,meanlog=paras$estimate[1],sdlog=paras$estimate[2] )
    v1= plnorm(x1,meanlog=paras$estimate[1],sdlog=paras$estimate[2] )
    v2= qlnorm(x2,meanlog=paras$estimate[1],sdlog=paras$estimate[2] )
  }    
  
  results<-list (v_cdf=v_cdf,v1=v1,v2=v2 , chi=chi)
  return(results)
  
  
}

setwd("C:\\Users\\ADMIN\\Desktop\\machine_learning")
destroyX = function(es) {
  f = es
  for (col in c(1:ncol(f))){ #for each column in dataframe
    if (startsWith(colnames(f)[col], "X") == TRUE)  { #if starts with 'X' ..
      colnames(f)[col] <- substr(colnames(f)[col], 2, 100) #get rid of it
    }
  }
  assign(deparse(substitute(es)), f, inherits = TRUE) #assign corrected data to original name
}
library(reshape2)
library(lubridate)
library(xts)
library(tsbox)
library(zoo)
library(ggplot2)
library(caret)
runoff=read.table("runoff.txt",header=T, na.strings = "NA", sep="\t")
runoff = destroyX(runoff)
R = melt(runoff,id.vars=c("Year","month"))
colnames(R) = c("Year","Month","Day","Q")
date = as.data.frame(with(R, ymd(paste(Year, Month, Day, sep= ' '))))
R1 = cbind(R,date)
R1 = na.omit(R1)
colnames(R1) = c("Year","Month","Day","Q","Date")
R1 = R1[order(as.Date(R1$Date, format="%d/%m/%Y")),]
R1[,4] <- sapply(R1[,4],as.numeric)
R1 = na.omit(R1)

station2 = xts(x=R1[,"Q"], order.by=R1[,"Date"])
station3=station2
month_seris= seq(as.Date("1989/1/1"), as.Date("2011/12/31"), "month")
station2<-xts:::xts(as.vector(R1$Q), order.by=month_seris)

index1=4 #for the dimension 
inital1=00

for (jj1 in 0:2) { #from lag 1 to lag 3 month ahead 
  lag=jj1  
  length1=length(station3)
  b1=as.numeric(station3[(index1+lag):(length1)])
  for (i in  (index1-1):1) { # the first colomn is the predicted one 
    
    # b0=as.numeric(station3[(i+lag):(length1+i-4-lag)])
    b0=as.numeric(station3[i:(length1+i-4-lag)])
    b1=cbind(b1, b0)
  }
  b1 = as.data.frame(b1)
  head(b1)
  
  
  
  b_cdf={}
  for (jj in 1:index1)
  { 
    b_cdf=cbind( b_cdf, best_fit_dist(b1[,jj])$v_cdf)
  }
  
  #b_cdf[,]
  
  #data=cbind(b_cdf[,4], b_cdf[,1], b_cdf[,2], b_cdf[,3])
  #data=cbind(b_cdf[,4], b_cdf[,3], b_cdf[,1], b_cdf[,2])
  
  # daily time series 
  #month_seris= seq(as.Date("1989/1/4"), as.Date("2011/12/31"), "days")
  
  # monthly time series 
  #month_seris= seq(as.Date("1989/4/1"), as.Date("2011/12/31"), "months")
  month_seris= seq(as.Date("1989/1/4"), as.Date("2011/12/31"), "days")
  #month_seris=format(month_seris, "%Y%m%d")
  
  aa1<-xts:::xts((b_cdf[,]), order.by=month_seris)
  
  data=aa1
  data1=aa1['1989/2006']
  colnames(data1) <- c("Y1","Y2","X3","X4")
  colnames(data) <- c("Y1","Y2","X3","X4")
  
  
  # for 4-d case  
  ListVines <- CDVineCondListMatrices(data,Nx=3,"CVine")
  
  final.ouput={}
  
  for (i in  1:length(ListVines$CVine)) {
    Matrix=ListVines$CVine[[i]]
    Matrix
    
    RVM <- CDVineCondFit(data1,Nx=3,Matrix=Matrix)
    summary(RVM)
    RVM$Matrix
    RVM$family
    # check
    identical(RVM$Matrix,Matrix)
    #  plot(RVM)
    
    
    
    
    d=dim(RVM$Matrix)[1]
    cond1 <- data[,RVM$Matrix[(d+1)-1,(d+1)-1]]
    cond2 <- data[,RVM$Matrix[(d+1)-2,(d+1)-2]]
    cond3 <- data[,RVM$Matrix[(d+1)-3,(d+1)-3]]
    condition <- cbind(cond1,cond2, cond3)
    
    Sim1={}
    for (m1 in 1:50) { 
      Sim0 <- CDVineCondSim(RVM,condition)
      head(Sim0)
      Sim1=cbind(Sim1,Sim0[,1])
    }
    
    Sim2=rowMeans(Sim1)
    
    Sim2 <- data.frame(Sim2)
    #data<-as.matrix(data)
    # overplot(Sim,as.data.frame(b_cdf))
    
    
    final=best_fit_dist(b1[,1], x2=Sim2[,1])$v2
    final.ouput=cbind(final.ouput,final)
    print(i)
  }
  
  
  newdata_v0<-xts:::xts((b1[,]), order.by=month_seris)
  
  final.ouput1<-xts:::xts((final.ouput[,]), order.by=month_seris)
  #newdata_v=newdata_v0['2006/2011'][, 1]
  #final.ouput2=final.ouput1['2006/2011'][, ]
  newdata_v=newdata_v0 [, 1]
  final.ouput2=final.ouput1 [, ]
  
  cor(final.ouput2[,5],newdata_v[,1])
  plot(final.ouput2[,3],newdata_v[,1])
  
  
  
  
  trainmonths=7 #could be changed 
  month_seris1=row.names(data.frame(final.ouput2[,1]))
  month_seris1=format(as.Date(month_seris1), "%Y%m%d")
  TestData <- ensembleData( forecasts = data.frame(final.ouput2) ,
                            dates = month_seris1   ,
                            observations = as.numeric(newdata_v),
                            forecastHour = inital1,
                            initializationTime = "00")
  
  TestFit <- ensembleBMAgamma( TestData, trainingDays = trainmonths,control = controlBMAgamma(startupSpeed = 1))
  
  TestForc <- quantileForecast( TestFit, TestData,quantiles = c( 0.025,0.05,0.166,0.5,0.833,0.95,0.975))
  
  # cor(tempTestForc[,4],newdata_v[traindays:length(newdata_v),1])
  #plot(tempTestForc[,4], newdata_v[traindays:length(newdata_v),1])
  month_seris1=row.names(data.frame(TestForc[,1]))
  TestForc1=xts:::xts(TestForc[,], order.by=as.Date(month_seris1))
  
  all_forecast=cbind(obs=newdata_v , 
                     final.ouput2 ,
                     TestForc1  )
  write.csv(all_forecast , file = file.path(paste("...\\",
                                                  "Test_output_lag",jj1,".csv", sep="")),
            sep=",", col.names = T, row.names = F ,append=F)
  
}
